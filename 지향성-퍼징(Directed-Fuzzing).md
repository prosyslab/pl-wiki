## 개요
지향성 퍼징은 퍼징 기법의 한 종류로서 프로그램의 특정 지점에 도달하는 입력을 생성하는 것을 목표로 한다.
예를 들어 패치나 정적분석 결과를 검사해야 할 때, 우리는 프로그램 전체보다는 주어진 패치나 정적분석 알람과 연관된 프로그램 지점에 관심을 가지게 된다.
이 때, 일반적인 퍼징 기법을 사용한다면 전체 프로그램을 살펴보느라 정작 우리가 관심있는 프로그램 지점은 제대로 검사하지 못할 수 있다.
따라서 이런 경우에는 주어진 프로그램 지점에 집중할 수 있는 지향성 퍼징이 더 효과적일 것이다.

## 퍼징
지향성 퍼징에 대해 더 자세히 설명하기 위해서는 그 상위 개념인 퍼징에 대해서 간략히 알고 갈 필요가 있다.
퍼징은 자동으로 무수한 입력을 생성하여 프로그램을 테스트하는 기술이다.
특히 무수한 입력을 무작위성의 힘을 빌려 생성하기 때문에 그 중에는 개발자가 미처 예상하지 못한 입력도 있기 마련이고, 이를 통해 미처 몰랐던 결함을 발견할 수도 있다.
퍼징은 대상 프로그램에 대한 정보가 거의 없는 상황에서도 프로그램을 효과적으로 관통하는 입력을 만들어내는 등, 그 강력한 성능으로 인해 많은 사랑을 받아왔다.

#### 퍼징의 탄생<sup>[1](#fuzzing_begin)</sup>
비바람이 몰아치던 어느날 밤, Wisconsin-Madison 대학의 Miller 교수는 집에서 야근을 하고 있었다.
야근하는 것도 서러운데 그날따라 학교 서버에서 원격으로 돌아가는 유닉스 유틸리티 프로그램들에 자꾸만 알 수 없는 오류가 발생했다고 한다.
하지만 뼛속까지 학자였던 Miller 교수는 그 원인을 추측하기에 나섰는데, 그것은 바로 번개로 인해 발생한 네트워크상의 노이즈가 자신의 정상적인 입력을 변형시켰고,
그것이 모종의 이유로 프로그램들의 오류를 유발했다는 것이었다.
그 해 Miller 교수는 자신의 운영체제 수업에서 이 가설을 실제로 검증하기 위한 과제를 내었고, 이것이 바로 퍼징의 시초이다.

#### 대표적인 퍼징 도구
가장 많이 알려진 대표적인 퍼징 도구는 단언컨대 AFL<sup>[2](#AFL)</sup> 일 것이다. 
AFL은 변형 기반 반투명(Mutation-Based Greybox) 퍼징 도구이다.
변형 기반 반투명 퍼징은 주어진 입력을 변형하여 새로운 입력을 만들고, 그 새로운 입력의 프로그램의 실행 결과를 엿보아 그것이 좋은 입력인지 아닌지 판단하는 것이다.
이때, 반투명(Greybox)이라 함은 프로그램 실행을 자세히 지켜보지는 않되 그 결과로 나오는 실행 커버리지 정도의 정보는 확인한다는 것을 뜻한다.
참고로 프로그램 내부를 샅샅히 들여다보면 투명(Whitebox), 아예 들여다보지 않고 프로그램의 입력과 출력만 본다면 불투명(Blackbox) 퍼징이라 한다.
또한 좋은 입력인지 아닌지는 보통 새로운 커버리지를 달성하였는지에 따라 결정을 하며, 좋은 입력의 경우 새로운 입력을 만들 재료로서 선택된다.

즉, 다음과 같은 순서로 퍼징이 진행된다.

0. 사용자가 주는 입력으로 초기 입력 집합 구성 (없다면 빈 입력으로 시작)
1. 입력을 하나 선택
2. 해당 입력을 변형하여 새로운 입력들 생성
3. 새로운 입력들로 프로그램 실행
4. 각각의 커버리지 결과를 바탕으로 입력 집합에 넣을 것인지, 아니면 폐기할 것이지 결정
5. 1번부터 다시 시작.

AFL의 이름의 유래는 American Fuzzy Lop이라 불리는 털이 복실복실한 토끼 품종이다.
이런 이름은 AFL이 가지는 퍼징의 특성을 잘 드러낸다.
왜냐하면 AFL은 토끼의 솜털같이 삐죽빼죽 튀어나오는 다양한 입력들을 통해 삐죽빼죽하게 프로그램의 구조를 탐색해 나가기 때문이다.
재미있는 점은 또 다른 퍼징 도구인 Angora<sup>[3](#Angora)</sup>  역시 앙고라 토끼의 이름을 따왔다는 것인데,
그 이유는 앙고라 토끼의 털이 AFL 토끼보다 더 빽빽하고 긴 것처럼 Angora가 프로그램을 더 깊고 넓게 탐색하기 때문이라고 한다.

## 지향성 퍼징의 역사
초기의 지향성 퍼징은 KATCH<sup>[4](#KATCH)</sup> 등의 도구처럼 기호실행(Symbolic Execution)을 활용한 방법들로 제안되었다.
그러나 기호실행은 프로그램 크기가 커질수록 속도가 많이 느려지기에 그다지 빛을 발하지 못했다.
하지만 2017년에 AFLGo라는 논문이 등장하며 본격적으로 지향성 퍼징의 역사가 시작되었다.
전체 프로그램이 아닌 특정 지점에 집중하는 퍼징은 정적 분석등의 결과를 검사하거나 패치를 검사할 때 유용하다는 점에서 많은 연구자들의 관심을 끌었다.
특히 연속개발(Continuous integration)과 같은 환경에서는 프로그램 수정이 빈번하게 일어나기에 수정된 부분에만 집중하는 지향성 퍼징은 희소식이 되었다.
따라서 활발한 연구가 진행되었고 2022년에 이르러서는 보안 분야의 최고 수준 학회(Security and Privacy)와 소프트웨어공학 분야의 최고 수준 학회(ICSE)에
지향성 퍼징 논문이 각각 한편씩 나오게 되었다.

## 주목할 만한 지향성 퍼징 도구들
- AFLGo<sup>[5](#AFLGo)</sup>:
최초의 AFL기반, 즉 변형기반 엿보기 퍼징에 지향성을 추가한 퍼징 도구이다.
AFLGo는 각 입력이 목표지점과 얼마나 가까운 프로그램 지점들을 지났는지를 고려한다.
이 때 프로그램 지점들 사이의 거리를 계산하기 위해 실행흐름그래프(Control Flow Graph)를 사용하게 된다.
실행흐름그래프 상에서 목표지점과 더 가까운 프로그램 지점을 실행한 입력이 있다면, 해당 입력으로부터 더 많은 새로운 입력을 생산하는 것이다.
즉, 위의 변형기반 반투명 퍼징의 순서에서 2번 단계를 건드리는 것이다.

- Beacon<sup>[6](#Beacon)</sup>:
핵심 아이디어는 가망이 없는 입력의 실행을 조기에 종료하는 것이다.
Beacon은 우선 프로그램 분석을 통해 각 프로그램 지점에서 목표 지점에 도달하기 위해 미리 만족해야 하는 조건을 찾아낸다.
그리고 입력의 실행 시 그 조건을 만족하지 못한다면 굳이 더 볼 필요가 없으니 미리 프로그램을 종료하는 것이다.
이것은 위의 변형기반 반투명 퍼징의 순서에서 3번 단계를 건드리는 것이다.

- DAFL<sup>[7](#DAFL)</sup>:
[관련 문서](https://github.com/prosyslab/pl-wiki/wiki/DAFL) 참고.

이 도구들의 공통점은 동적으로 프로그램을 검사하는 대표적인 기법인 퍼징에 프로그램 분석, 즉 정적으로 프로그램을 검사하는 기법을 적용했다는 점이다.


## 지향성 퍼징 도구의 평가 시 주의점
위에서 보았듯이 지향성 퍼징 분야는 활발히 연구되고 있으며 다양한 접근들이 이루어지고 있다.
하지만 안타깝게도 신생 연구분야인 지향성 퍼징은 한동안 이렇다 할 만한 연구의 표준이 정리되지 않아 성능 평가가 일관되게 이루어지지 않고 있었다.
이러한 문제점을 극복하기 위한 노력의 일환으로 최근 지향성 퍼징 도구의 성능 평가의 기준이 제안되었고<sup>[8](#EvalDirFuzz)</sup> 그 내용은 다음과 같다.

지향성 퍼징 도구는 일반적으로 결함 재현 실험을 통해 성능을 평가한다.
결함 재현 실험은 알려진 결함을 얼마나 빨리 재현하는지를 평가하는 실험으로 다음과 같은 단계로 이루어진다.

- 목표 결함을 대표하는 프로그램 지점을 선정하여 지향성 퍼징 도구에 입력으로 전달하고, 주어진 시간동안 퍼징 도구를 실행한다.
- 실행이 종료되면 그 과정에서 찾은 결함들이 목표한 결함인지 확인한다.
- 확인된 결함들을 기록하고, 목표 결함을 재현하는데 걸린 시간을 기록한다.
- 결과의 신뢰성을 높이기 위해 이 과정을 일정한 횟수만큼 반복한다.
- 두 도구의 성능을 비교하기 위해 목표 결함을 재현하는데 걸린 시간의 중앙값을 비교하거나 보다 정교한 통계 검사를 한다.

위의 각 단계에서 어떤 선택을 하는지에 따라 성능 평가의 결과가 크게 달라질 수 있다. 구체적으로 각 단계에서 어떤 문제가 발생할 수 있으며 이를 방지하기 위해 어떤 노력을 해야 하는지 알아보자.

### 1. 목표 선정

결함 재현 실험은 보통 CVE(Common Vulnerabilities and Exposures)등의 알려진 결함을 목표 결함으로 사용한다.
하지만 하나의 CVE에 대해 어떤 목표 지점을 구체적으로 지향성 퍼징 도구에 전달해야 하는지는 명확하게 정해져 있지 않다.
실제로 같은 CVE에 근본적인 원인이 다른 두 결함이 묶여있을 수 있고, 결함이 발생한 경로 중 어떤 지점이 결함의 맥락을 가장 잘 나타내는지도 명확하지 않다.
문제는 이렇게 목표 지점이 달라지면 결함 재현 실험의 결과도 달라진다는 것이다.
따라서 여러 연구에 걸쳐 일관적인 평가를 위해서는 단순히 사용한 CVE 번호만을 공개하는 것이 아닌 실제 사용한 목표 지점을 공개해야 한다.

### 2. 목표 결함 확인

주어진 시간동안 발견된 결함들이 목표 결함인지 확인하는 방법에는 크게 두 가지가 있다.
먼저 프로그램 안전벨트(Sanitizer)를 사용하여 발견된 결함들을 검사하는 방법이 있다.
보통 프로그램 안전벨트는 발생한 결함의 종류와 스택 트레이스를 제공하는데, 이 정보가 목표 결함의 정보와 일치하는지를 확인하여 결함을 확인하는 것이다.
하지만 정보가 얼마나 일치하는지에 대한 기준이 명확하지 않다.
예를 들어 스택 트레이스가 모두 일치해야만 같은 결함일 수 있지만 결함 발생 지점만 일치해도 같은 결함인 경우도 있다. 심지어 결함 발생 지점이 다르더라도 같은 결함일 수도 있다.

좀 더 명확한 방법으로는 목표 결함을 수정하는 패치를 사용하는 방식이 있다.
즉, 발견된 결함을 유발하는 입력들이 패치가 적용된 프로그램에서 정상적으로 실행된다면 해당 결함은 목표 결함이라고 판단하는 것이다.
얼핏 확실한 방법처럼 보이지만, 이 또한 몇 가지 문제점을 가지고 있다.
먼저 어떤 패치가 목표 결함을 완전히 수정했는지를 판단하는 것이 어렵다.
왜냐하면 여러 패치에 걸쳐 하나의 결함이 수정될 수 있고, 하나의 패치가 여러 결함을 수정할 수도 있기 때문이다.
또한 이중으로 결함이 존재하는 경우, 패치가 적용된 프로그램에서 기존 결함은 사라지지만 새로운 결함이 발생할 수 있다.

따라서 일관적인 평가를 위해 최소한 구체적인 목표 결함 검사 방법을 공개해야 한다.
더 나아가 애초에 검사 로직이 포함된 벤치마크를 사용하는 것이 바람직하다.
예를 들어 목표 결함 검사문(assertion)이 삽입되어 있는 프로그램이나, 단 하나의 결함만 존재하도록 합성된 프로그램<sup>[9](#Fuzzle)</sup>이 여기에 해당한다.

### 3. 결함 발견 시간 측정

결함 발견 시간은 보통 TTE(Time To Exploit)으로 표기한다.
이 때 대부분의 연구는 퍼징 시간만 고려하여 TTE를 산출해내었다.
하지만 지향성 퍼징의 경우 유독 많은 도구들이 퍼징에 앞서 프로그램을 분석하고, 분석 결과를 이용하여 퍼징을 진행한다.
심지어 이 분석 시간이 퍼징 시간보다 더 오래 걸리는 경우도 있다.
프로그램 분석은 보통 프로그램이 수정될 때마다 새로 해야 하기 때문에 지속적인 개발 환경 등에서 지향성 퍼징을 활용하려면 이러한 분석 시간도 함께 고려되어야 한다.
따라서 분석 시간도 함께 공개하여 보다 다양한 측면에서 도구들을 비교할수 있도록 해야 한다.

### 4. 실험 반복 횟수

퍼징은 무작위성을 이용하여 입력을 생성하기 때문에 항상 조금씩 다른 결과가 나온다.
프로그램 내의 탐색 영역이 매번 달라질 수 있기 떄문이다.
따라서 퍼징 도구를 평가할 때는 실험을 여러 번 반복하여 그 결과의 신뢰성을 높인다.
여러 번의 실험을 반복하면 커버리지 달성률이나 발견된 결함의 갯수 등이 어느정도 수렴하기 때문이다.

하지만 지향성 퍼징의 경우 프로그램 전체에서 발생하는 커버리지나 결함의 갯수는 중요하지 않다.
왜냐하면 특정 지점에 도달하여 특정 결함을 재현하는 것이 목표이기 때문이다.
따라서 지향성 퍼징은 일반적인 퍼징보다 무작위성에 더욱 취약하여 더 많은 횟수의 실험을 필요로 한다.
실제로 최근 연구에서는 40회 이상의 실험을 반복하여 그 결과를 평가하였다.

### 5. 통계 검증

성능 평가의 최종 단계에서는 여러번 반복한 실험 결과를 통해 둘 이상의 퍼징도구의 성능을 비교해야 한다.
이런 성능 비교는 TTE의 중앙값을 비교하거나 Mann-Whitney U(MWU) 검사와 같은 보다 정확한 통계 검사를 하기도 한다.
하지만 지향성 퍼징의 경우 주어진 시간안에 목표 결함을 발견하지 못하는 타임 아웃이 발생할 수 있는데, MWU 검사는 근본적으로
이러한 데이터를 적절하게 처리할 수 없다.
따라서 이전 지향성 퍼징 연구는 예를 들어, 24시간 실험동안 목표 결함을 발견하지 못한 경우에 결함 발견 시간을 24시간으로 간주하는 방식으로 MWU 검사를 수행했다. 하지만 이는 타임 아웃이 발생한 퍼징 도구의 성능을 과대 평가하는 결과를 낳을 수 있기에 적절하지 않다.

지향성 퍼징의 경우, MWU 검사보다는 Log-rank 검사 등의 생존분석(survival analysis)을 사용하는 것이 바람직하다.
생존 분석의 경우 주어진 시간 내에 사건이 관찰되지 않는 경우도 처리할 수 있기에 타임 아웃이 발생하는 지향성 퍼징에 적합하기 때문이다.
특히 성능을 시각화 할수 있도록 선인장 그래프 (cactus plot)도 함께 사용한다면 금상첨화일 것이다.



## 각주

[<a name="fuzzing_begin">1</a>] https://www.cs.wisc.edu/2021/01/14/the-trials-and-tribulations-of-academic-publishing-and-fuzz-testing/

[<a name="AFL">2</a>] https://afl-1.readthedocs.io/en/latest/index.html

[<a name="Angora">3</a>] "Angora: Efficient Fuzzing by Principled Search", Chen and Chen, S&P 2018

[<a name="KATCH">4</a>] "KATCH: High-Coverage Testing of Software Patches", Marinescu and Cadar, FSE 2013

[<a name="AFLGo">5</a>] "Directed Greybox Fuzzing", Bohme et al., CCS 2017

[<a name="Beacon">6</a>] "BEACON : Directed Grey-Box Fuzzing with Provable Path Pruning", Huang et al., S&P 2022

[<a name="DAFL">7</a>] "[DAFL : Directed Grey-Box Fuzzing Guided by Data Dependency](https://prosys.kaist.ac.kr/publications/sec23.pdf)", Kim et al., USENIX Security 2023

[<a name="EvalDirFuzz">8</a>] "Evaluating Directed Fuzzers: Are We Heading in the Right Direction?", Kim et al., FSE 2024

[<a name="Fuzzle">9</a>] "Fuzzle: Making a Puzzle for Fuzzers", Lee, Kim, and Cha, ASE 2022